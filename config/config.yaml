base:
  # tiny_model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  # large_model_id: "meta-llama/Llama-2-7b-chat-hf"
  tiny_model_id: "Qwen/Qwen1.5-0.5B-Chat"
  large_model_id: "Qwen/Qwen1.5-14B-Chat"
  # tiny_model_id: "Qwen/Qwen2.5-0.5B-Instruct"
  # large_model_id: "Qwen/Qwen2.5-14B-Instruct"
  lamp5_path: "data/Lamp5/402.json"
  lamp4_path: "data/Lamp4/example.json"
  lamp3_path: "data/Lamp3/example.json"
  ##Lamp5
  model_path: "results/models/20250927_081149_Qwen1.5-0.5B-Chat_merged"
  # large_model_path: "results/models/20250927_073000_Qwen1.5-7B-Chat_merged"
  large_model_path: "results/models/20250927_092539_Qwen1.5-14B-Chat_merged"
  # model_path: "results/models/20250924_034209_Qwen2.5-0.5B-Instruct_merged"
  # large_model_path: "results/models/20250924_132022_Qwen2.5-14B-Instruct_merged"

  fusion_network_path: "results/models/combModel/20250927081612/checkpoint_epoch4.pt"
  device_id: "2"
  max_length: 35
  top_k: 50
  temperature: 0.1
  
# 训练配置
tinyModel_training:
  train_type: "large"
  output_dir: "results/models"
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  num_epochs: 1
  max_steps: 5
  save_total_limit: 1  

  fp16: true
  
# LoRA配置
lora:
  r: 24
  alpha: 48
  dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj"]

combModel_training:
  batch_size: 4
  # lr: 0.0002
  lr: 0.0002
  output_dir: "results/models/combModel"
  max_length: 512
  num_epochs: 10

replay:
  balancing_weight: 0.9